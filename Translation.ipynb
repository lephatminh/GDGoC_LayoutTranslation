{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11de210a",
   "metadata": {},
   "source": [
    "# Layout Detection +  Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524c1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import ocrmypdf\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect, DetectorFactory\n",
    "from PyPDF2 import PdfReader, PdfWriter, Transformation\n",
    "import copy\n",
    "import string\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e227e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"ocr_recovery.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2fbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_csv_field_size():\n",
    "    \"\"\"Find the maximum CSV field size limit using binary search\"\"\"\n",
    "    max_int = 2147483647  # 2^31-1\n",
    "    min_int = 1024\n",
    "    \n",
    "    while min_int < max_int:\n",
    "        try:\n",
    "            mid = (min_int + max_int + 1) // 2\n",
    "            csv.field_size_limit(mid)\n",
    "            min_int = mid\n",
    "        except OverflowError:\n",
    "            max_int = mid - 1\n",
    "    \n",
    "    return min_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49727319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Safely set maximum CSV field size limit\n",
    "csv.field_size_limit(find_max_csv_field_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d21fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_rgb(color_int):\n",
    "    \"\"\"Convert integer color to RGB tuple.\"\"\"\n",
    "    if color_int < 0:\n",
    "        color_int = color_int & 0xFFFFFFFF\n",
    "\n",
    "    a = (color_int >> 24) & 0xFF\n",
    "    r = (color_int >> 16) & 0xFF\n",
    "    g = (color_int >> 8) & 0xFF\n",
    "    b = color_int & 0xFF\n",
    "\n",
    "    if (a == 0):\n",
    "        a = 255\n",
    "\n",
    "    return [r, g, b, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a581e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spaced_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text with excessive spacing between characters,\n",
    "    commonly found in headers like \"F I N A N C I A L  S T A T E M E N T S\" or \"\"\n",
    "    \"\"\"\n",
    "    # Check if text has consistent spacing pattern (every character followed by space)\n",
    "    if len(text) > 3 and all(text[i] == ' ' for i in range(1, len(text), 2)):\n",
    "        # Join characters by removing spaces\n",
    "        return ''.join(text[i] for i in range(0, len(text), 2))\n",
    "    \n",
    "    # Check if text has spaces between all characters\n",
    "    if len(text) > 3 and ' ' in text:\n",
    "        # Count spaces vs non-spaces\n",
    "        spaces = text.count(' ')\n",
    "        non_spaces = len(text) - spaces\n",
    "        \n",
    "        # If the ratio of spaces to characters is high (e.g., spaces >= characters)\n",
    "        if spaces >= non_spaces - 1:\n",
    "            text_split = text.split(' ')\n",
    "            text_split = [' ' if char == '' else char for char in text_split]\n",
    "            return ''.join(text_split)\n",
    "    \n",
    "    # Return original if no patterns match\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5e2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ocr_to_pdf(input_path, output_dir):\n",
    "    \"\"\"Apply OCR to a PDF file using OCRmyPDF\"\"\"\n",
    "    # Create OCR file in data/test/PDF_ocr directory\n",
    "    output_path = output_dir / f\"{input_path.stem}.ocr.pdf\"\n",
    "\n",
    "    try:\n",
    "        languages = [\n",
    "            \"chi_sim\",  # Simplified Chinese\n",
    "            \"chi_tra\",  # Traditional Chinese\n",
    "            \"vie\",      # Vietnamese\n",
    "            \"eng\",      # English\n",
    "            \"jpn\",      # Japanese\n",
    "            \"kor\",      # Korean\n",
    "            \"fra\",      # French\n",
    "            \"deu\",      # German\n",
    "            \"spa\",      # Spanish\n",
    "            \"rus\"       # Russian\n",
    "        ]\n",
    "\n",
    "        # Run OCR with multiple language support\n",
    "        ocrmypdf.ocr(\n",
    "            input_path,\n",
    "            output_path,\n",
    "            language=\"+\".join(languages),\n",
    "            deskew=True,\n",
    "            clean=False,\n",
    "            optimize=0,\n",
    "            output_type='pdf',  # Changed from 'pdfa' to 'pdf' to retain original color space\n",
    "            skip_text=True,\n",
    "            progress_bar=True,\n",
    "            color_conversion_strategy='UseDeviceIndependentColor',\n",
    "        )\n",
    "        logging.info(f\"OCR completed: {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OCR error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69096742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_language_code_for_deep_translator(lang_code):\n",
    "    mapping = {\n",
    "        \"zh-cn\": \"zh-CN\",\n",
    "        \"zh-hans\": \"zh-CN\",\n",
    "        \"zh-tw\": \"zh-TW\",\n",
    "        'zh-hant': 'zh-TW',\n",
    "        'zh': 'zh-CN',     # Default Chinese to Simplified\n",
    "        'jw': 'jv',        # Javanese\n",
    "        'iw': 'he',        # Hebrew\n",
    "        'in': 'id',        # Indonesian\n",
    "        'ceb': 'tl',       # Adjust Cebuano to use Tagalog\n",
    "    }\n",
    "\n",
    "    return mapping.get(lang_code, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb94801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate_text(texts_with_langs, target='vi', batch_size=25, delay=0):\n",
    "    \"\"\"\n",
    "    Translate a batch of texts with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to translate\n",
    "        source: Source language code\n",
    "        target: Target language code\n",
    "        batch_size: Number of texts to translate in one batch\n",
    "        delay: Delay between batches in seconds\n",
    "        \n",
    "    Returns:\n",
    "        List of translated texts\n",
    "    \"\"\"\n",
    "    results = [\"\"] * len(texts_with_langs)\n",
    "\n",
    "    # Group text by detected source language\n",
    "    lang_groups = {}\n",
    "    for text, lang, orig_idx in texts_with_langs:\n",
    "        if not lang in lang_groups:\n",
    "            lang_groups[lang] = []\n",
    "        lang_groups[lang].append((text, orig_idx))\n",
    "\n",
    "    for source_lang, texts_with_indices in lang_groups.items():\n",
    "        if source_lang == target:\n",
    "            for text, orig_idx in texts_with_indices:\n",
    "                results[orig_idx] = text\n",
    "            continue\n",
    "\n",
    "        texts = [t[0] for t in texts_with_indices]\n",
    "        indices = [t[1] for t in texts_with_indices]\n",
    "\n",
    "        # Create translator for this language\n",
    "        translator = GoogleTranslator(source=source_lang, target=target)\n",
    "\n",
    "        translated_batch = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:min(i + batch_size, len(texts))]\n",
    "            \n",
    "            # Process each text in the current batch\n",
    "            batch_results = []\n",
    "            for text in batch:\n",
    "                try:    \n",
    "                    translated = translator.translate(text)\n",
    "                    batch_results.append(translated)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Translation error: {str(e)[:100]}...\")\n",
    "                    # Return original text on error\n",
    "                    batch_results.append(text)\n",
    "                    \n",
    "                    # Handle rate limiting - increase delay and reduce batch size\n",
    "                    if \"429\" in str(e) or \"too many requests\" in str(e).lower():\n",
    "                        logging.info(f\"Rate limit hit. Increasing delay to {delay*2}s and reducing batch size.\")\n",
    "                        delay *= 2\n",
    "                        batch_size = max(1, batch_size // 2)\n",
    "                        time.sleep(5)  # Additional pause after hitting rate limit\n",
    "            \n",
    "            translated_batch.extend(batch_results)\n",
    "            \n",
    "            # Add delay between batches\n",
    "            if i + batch_size < len(texts):\n",
    "                time.sleep(delay)\n",
    "\n",
    "        # Put translated texts back in their original positions\n",
    "        for translated_text, orig_idx in zip(translated_batch, indices):\n",
    "            results[orig_idx] = translated_text\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af9ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_cells(cells, target='vi'):\n",
    "    \"\"\"\n",
    "    Translate text in cells from source language to target language.\n",
    "    \n",
    "    Args:\n",
    "        cells: List of cell dictionaries with text\n",
    "        source: Source language code\n",
    "        target: Target language code\n",
    "        \n",
    "    Returns:\n",
    "        List of cell dictionaries with translated text\n",
    "    \"\"\"\n",
    "    # Extract all texts and detect languages\n",
    "    texts_with_langs = []\n",
    "    for i, cell in enumerate(cells):\n",
    "        if cell.get(\"text\"):\n",
    "            try:\n",
    "                # Detect language for each text\n",
    "                lang = detect(cell[\"text\"])\n",
    "                # Map language code for deep_translator\n",
    "                mapped_lang = map_language_code_for_deep_translator(lang)\n",
    "                # Store original language in cell\n",
    "                texts_with_langs.append((cell[\"text\"], mapped_lang, i))\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Language detection error: {str(e)[:100]}... Using 'en' as fallback.\")\n",
    "                texts_with_langs.append((cell[\"text\"], \"en\", i))\n",
    "    \n",
    "    logging.info(f\"Translating {len(texts_with_langs)} text segments to {target}...\")\n",
    "\n",
    "    lang_counts = {}\n",
    "    for _, lang, _ in texts_with_langs:\n",
    "        lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
    "    \n",
    "    logging.info(\"Detected languages:\")\n",
    "    for lang, count in lang_counts.items():\n",
    "        logging.info(f\"  - {lang}: {count} segments\")\n",
    "    \n",
    "    # Perform batch translation\n",
    "    translated_texts = batch_translate_text(texts_with_langs, target)\n",
    "    \n",
    "    # Map translated texts back to cells\n",
    "    text_index = 0\n",
    "    for cell in cells:\n",
    "        if cell.get(\"text\"):\n",
    "            cell[\"text_vi\"] = translated_texts[text_index]\n",
    "            text_index += 1\n",
    "    \n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ccf648",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = [ {'x': 72.000732421875,\n",
    "  'y': 470.55364990234375,\n",
    "  'width': 388.0931396484375,\n",
    "  'height': 11.9671630859375,\n",
    "  'text': 'due to the high thermal excitations in the hot and very dilute nuclear matter (i.e. µ',#'Since the new nuclear matter turns to be dominated by the colorless U (1)',\n",
    "  'font': {'color': [0, 0, 0, 255], 'name': 'CMR12', 'size': 11},\n",
    "  'text_vi': 'Since the new nuclear matter turns to be dominated by the colorless U (1)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bcfe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:09:31,107 [INFO] Translating 1 text segments to vi...\n",
      "2025-05-08 15:09:31,108 [INFO] Detected languages:\n",
      "2025-05-08 15:09:31,109 [INFO]   - en: 1 segments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'x': 72.000732421875,\n",
       "  'y': 470.55364990234375,\n",
       "  'width': 388.0931396484375,\n",
       "  'height': 11.9671630859375,\n",
       "  'text': 'due to the high thermal excitations in the hot and very dilute nuclear matter (i.e. µ',\n",
       "  'font': {'color': [0, 0, 0, 255], 'name': 'CMR12', 'size': 11},\n",
       "  'text_vi': 'Do sự kích thích nhiệt cao trong chất hạt nhân nóng và rất loãng (tức là'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_cells(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9c1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing/replacing non-printable characters\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Replace common problematic Unicode characters\n",
    "    replacements = {\n",
    "        '\\u0000': '',  # NULL\n",
    "        '\\u0001': '',  # START OF HEADING\n",
    "        '\\u0002': '',  # START OF TEXT\n",
    "        '\\u0003': '',  # END OF TEXT\n",
    "        '\\u0004': '',  # END OF TRANSMISSION\n",
    "        '\\u0005': '',  # ENQUIRY\n",
    "        '\\u0006': '',  # ACKNOWLEDGE\n",
    "        '\\u0007': '',  # BELL\n",
    "        '\\u0014': '',  # DEVICE CONTROL FOUR\n",
    "        '\\u0015': '',  # NEGATIVE ACKNOWLEDGE\n",
    "        '\\ufffd': '',  # REPLACEMENT CHARACTER (�)\n",
    "        '\\u200b': '',  # ZERO WIDTH SPACE\n",
    "        '\\u200e': '',  # LEFT-TO-RIGHT MARK\n",
    "        '\\u200f': '',  # RIGHT-TO-LEFT MARK\n",
    "        '\\ufeff': '',  # ZERO WIDTH NO-BREAK SPACE\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for char, replacement in replacements.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    \n",
    "    # Filter out any remaining control characters\n",
    "    return ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ed52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_info(pdf_path):\n",
    "    \"\"\"Extract text and formatting information from a PDF file\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    cells = []\n",
    "    \n",
    "    try:\n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        span[\"text\"] = clean_text(span[\"text\"]).strip()\n",
    "                        # Skip empty spans\n",
    "                        if not span[\"text\"]:\n",
    "                            continue\n",
    "\n",
    "                        # Normalize text with excessive spacing\n",
    "                        normalized_text = normalize_spaced_text(span[\"text\"])\n",
    "\n",
    "                        cell = {\n",
    "                            \"x\": span[\"bbox\"][0],\n",
    "                            \"y\": span[\"bbox\"][1],\n",
    "                            \"width\": span[\"bbox\"][2] - span[\"bbox\"][0],     # width\n",
    "                            \"height\": span[\"bbox\"][3] - span[\"bbox\"][1],    # height\n",
    "                            \"text\": normalized_text,\n",
    "                            \"font\": {\n",
    "                                \"color\": int_to_rgb(span[\"color\"]),\n",
    "                                \"name\": span[\"font\"],\n",
    "                                \"size\": int(span[\"size\"]),\n",
    "                            },\n",
    "                            \"text_vi\": normalized_text  # Will be translated later\n",
    "                        }\n",
    "                        cells.append(cell)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
    "    finally:\n",
    "        doc.close()\n",
    "    \n",
    "    # Add translation step\n",
    "    if cells:\n",
    "        try:\n",
    "            logging.info(f\"Translating {len(cells)} cells to Vietnamese...\")\n",
    "            cells = translate_cells(cells, target='vi')\n",
    "            logging.info(f\"Translation complete for {len(cells)} cells\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Translation error: {str(e)}\")\n",
    "            # Continue with untranslated text\n",
    "\n",
    "    # Final null check before returning\n",
    "    for cell in cells:\n",
    "        if cell.get(\"text_vi\") is None:\n",
    "            cell[\"text_vi\"] = cell.get(\"text\", \"\")  # Use original or empty string\n",
    "\n",
    "    return {\"cells\": cells}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcba6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_cells(pdf_path):\n",
    "    \"\"\"Extract text and formatting information from a PDF file without translation.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    cells = []\n",
    "    \n",
    "    try:\n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        # Clean and normalize text\n",
    "                        span_text = clean_text(span[\"text\"]).strip()\n",
    "                        # Skip empty spans\n",
    "                        if not span_text:\n",
    "                            continue\n",
    "\n",
    "                        # Normalize text with excessive spacing\n",
    "                        normalized_text = normalize_spaced_text(span_text)\n",
    "\n",
    "                        cell = {\n",
    "                            \"x\": span[\"bbox\"][0],\n",
    "                            \"y\": span[\"bbox\"][1],\n",
    "                            \"width\": span[\"bbox\"][2] - span[\"bbox\"][0],     # width\n",
    "                            \"height\": span[\"bbox\"][3] - span[\"bbox\"][1],    # height\n",
    "                            \"text\": normalized_text,\n",
    "                            \"font\": {\n",
    "                                \"color\": int_to_rgb(span[\"color\"]),\n",
    "                                \"name\": span[\"font\"],\n",
    "                                \"size\": int(span[\"size\"]),\n",
    "                            },\n",
    "                            \"text_vi\": normalized_text  # Set to \"vietnamese\" as requested\n",
    "                        }\n",
    "                        cells.append(cell)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    # Final null check for text fields\n",
    "    for cell in cells:\n",
    "        if cell.get(\"text\") is None:\n",
    "            cell[\"text\"] = \"\"\n",
    "        if cell.get(\"text_vi\") is None:\n",
    "            cell[\"text_vi\"] = cell[\"text\"]\n",
    "\n",
    "    return {'cells' : cells}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0cdde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text to standardize Unicode characters for mathematical and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized text with standardized characters and cleaned formatting.\n",
    "    \"\"\"\n",
    "    # Configure logging for warnings about unmapped characters\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "    # Define a mapping for problematic Unicode characters to their preferred forms\n",
    "    unicode_mapping = {\n",
    "        '\\u00B5': '\\u03BC',  # Micro Sign (µ) → Greek Small Letter Mu (μ)\n",
    "        '\\u2243': '\\u2248',  # Asymptotically Equal To (≃) → Almost Equal To (≈)\n",
    "        '\\u2245': '\\u2248',  # Approximately Equal To (≅) → Almost Equal To (≈)\n",
    "        '\\u2212': '\\u002D',  # Minus Sign (−) → Hyphen-Minus (-)\n",
    "        '\\u2013': '\\u002D',  # En Dash (–) → Hyphen-Minus (-)\n",
    "        '\\u2014': '\\u002D',  # Em Dash (—) → Hyphen-Minus (-)\n",
    "        '\\u00A0': '\\u0020',  # Non-Breaking Space → Regular Space\n",
    "        '\\u200B': '',        # Zero-Width Space → Remove\n",
    "        '\\uFEFF': '',        # Zero-Width No-Break Space (BOM) → Remove\n",
    "    }\n",
    "\n",
    "    # Apply NFKC normalization to handle decomposed forms and compatibility characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Apply custom Unicode mappings\n",
    "    for source_char, target_char in unicode_mapping.items():\n",
    "        if source_char in text:\n",
    "            logging.info(f\"Replacing {source_char} (U+{ord(source_char):04X}) with {target_char} (U+{ord(target_char):04X})\")\n",
    "            text = text.replace(source_char, target_char)\n",
    "\n",
    "    # Check for unmapped special characters and log warnings\n",
    "    special_chars = set(c for c in text if ord(c) > 127 and c not in unicode_mapping.values())\n",
    "    if special_chars:\n",
    "        logging.warning(f\"Found unmapped special characters: {special_chars}. Consider adding to unicode_mapping.\")\n",
    "\n",
    "    # Remove non-printable control characters (except spaces)\n",
    "    text = ''.join(c for c in text if c.isprintable() or c.isspace())\n",
    "\n",
    "    # Normalize multiple spaces, tabs, or newlines to a single space\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27a31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_visual_length(text, font_size = 11 ,font_name = 'NotoSansMath', font_file_path = './NotoSansMath-Regular.ttf'):\n",
    "\n",
    "    '''\n",
    "    font_file_path=\"./NotoSansMath-Regular.ttf\"\n",
    "    font_name = 'NotoSansMath'\n",
    "    '''\n",
    "\n",
    "    font = fitz.Font(fontname=font_name, fontfile=font_file_path)\n",
    "\n",
    "    # Measure and shrink font to fit the box\n",
    "    text_width = font.text_length(text, fontsize=font_size)\n",
    "\n",
    "    return text_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acfb1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_translation(pdf_path, font_file_path=\"Roboto.ttf\"):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF using OCR, save to CSV, and visualize text in a new PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF file.\n",
    "        font_file_path (str): Path to the custom font file (default: './Roboto.ttf').\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"document_translation.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract file ID from the PDF path (without extension)\n",
    "    file_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    # Create temporary working folder\n",
    "    working_folder = f\"./{file_id}\"\n",
    "    if os.path.exists(working_folder):\n",
    "        shutil.rmtree(working_folder)\n",
    "        logging.info(f\"Deleted existing working folder: {working_folder}\")\n",
    "    os.makedirs(working_folder, exist_ok=True)\n",
    "\n",
    "    # Copy the input PDF to the working folder\n",
    "    new_pdf_path = os.path.join(working_folder, os.path.basename(pdf_path))\n",
    "    shutil.copy(pdf_path, new_pdf_path)\n",
    "\n",
    "    # Define file paths\n",
    "    ocr_pdf_path = os.path.join(working_folder, f\"{file_id}.ocr.pdf\")\n",
    "    csv_file_path = os.path.join(working_folder, f\"{file_id}.csv\")\n",
    "    output_pdf_path = f\"./translation_{file_id}.pdf\"\n",
    "\n",
    "    # Step 1: Apply OCR to the PDF\n",
    "    logging.info(f\"Applying OCR to {new_pdf_path}...\")\n",
    "    ocr_result = apply_ocr_to_pdf(Path(new_pdf_path), Path(working_folder))\n",
    "    if not ocr_result:\n",
    "        logging.error(\"OCR failed. Exiting.\")\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "    logging.info(f\"OCR PDF saved to {ocr_result}\")\n",
    "\n",
    "    # Step 2: Extract cells using extract_pdf_cells\n",
    "    logging.info(f\"Extracting cells from {ocr_result}...\")\n",
    "    cells = extract_pdf_cells(ocr_result)[\"cells\"]\n",
    "    \n",
    "    if not cells:\n",
    "        logging.error(\"No cells extracted from PDF. Exiting.\")\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "    logging.info(f\"Extracted {len(cells)} cells\")\n",
    "\n",
    "    # Step 3: Save extracted cells to CSV\n",
    "    csv_data = [{\n",
    "        \"id\": file_id,\n",
    "        \"solution\": json.dumps(cells, ensure_ascii=False)\n",
    "    }]\n",
    "    try:\n",
    "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"id\", \"solution\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(csv_data)\n",
    "        logging.info(f\"Saved {len(cells)} cells to {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving CSV: {str(e)}\")\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "\n",
    "    # Step 4: Open the OCR PDF for modification\n",
    "    try:\n",
    "        doc = fitz.open(ocr_pdf_path)\n",
    "        doc_size = fitz.open(pdf_path)  # Open original PDF to get page size\n",
    "        page = doc[0]  # Modify the first page\n",
    "        page_size = doc_size[0]\n",
    "        page_rect = page_size.rect\n",
    "        page_width = page_rect.width\n",
    "        page_height = page_rect.height\n",
    "        logging.info(f\"Opened {ocr_pdf_path}, size: {page_width} x {page_height}\")\n",
    "        doc_size.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error opening PDF: {str(e)}\")\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "\n",
    "    # Step 5: Read the CSV and visualize cells in the PDF\n",
    "    try:\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            data = list(reader)\n",
    "        logging.info(f\"Read {len(data)} rows from {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV: {str(e)}\")\n",
    "        doc.close()\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "\n",
    "    for item in data:\n",
    "        try:\n",
    "            ocr_data = json.loads(item['solution'])\n",
    "            logging.info(f\"Processing item with id: {item['id']}, {len(ocr_data)} entries\")\n",
    "            \n",
    "            for entry in ocr_data:\n",
    "                x = entry[\"x\"]\n",
    "                y = entry[\"y\"]\n",
    "                width = entry[\"width\"]\n",
    "                height = entry[\"height\"]\n",
    "                text = normalize_text(entry[\"text\"]).strip()\n",
    "                text_vi = entry[\"text_vi\"]  # Placeholder \"vietnamese\" from extract_pdf_cells\n",
    "                # Optionally use original text if visualization requires it\n",
    "                # text_vi = entry[\"text\"]  # Uncomment to use original text instead\n",
    "                font_size = entry[\"font\"][\"size\"]\n",
    "                font_name = 'Roboto'\n",
    "                \n",
    "                #text_vi = '= 3, có khả năng vật chất Hagedorn trực giao trải qua bậc ba'\n",
    "\n",
    "                # width = text_visual_length(text) # No,we are override, this rectangele will smaller than the initial word\n",
    "\n",
    "                # Define the rectangle for the text\n",
    "                rect = fitz.Rect(x, y, x + width, y + height)\n",
    "\n",
    "                # Cover the original text with a white rectangle\n",
    "                #page.draw_rect(rect, color=[1, 1, 1], fill=[1, 1, 1])\n",
    "                page.draw_rect(rect, color=[0, 0, 0], fill=[1, 1, 1])\n",
    "\n",
    "                # Initialize the font object from file\n",
    "                font = fitz.Font(fontname=font_name, fontfile=font_file_path)\n",
    "\n",
    "                font_size = 20\n",
    "\n",
    "                # Measure and shrink font to fit the box\n",
    "                text_width = font.text_length(text_vi, fontsize=font_size)\n",
    "                #print(text,':',text_width)\n",
    "                while text_width > width and font_size > 1:\n",
    "                     font_size -= 1\n",
    "                     text_width = font.text_length(text_vi, fontsize=font_size)\n",
    "\n",
    "                # Center the text vertically in the rectangle\n",
    "                y_text_pos = y  + (height)\n",
    "\n",
    "                # Insert the text\n",
    "                page.insert_text(\n",
    "                    (x, y_text_pos),\n",
    "                    text_vi,\n",
    "                    fontsize=font_size,\n",
    "                    fontname=font_name,\n",
    "                    fontfile=font_file_path,\n",
    "                    encoding='utf-16',\n",
    "                    fill_opacity=1,\n",
    "                    stroke_opacity=1,\n",
    "                    border_width=1\n",
    "                )\n",
    "                logging.info(f\"Visualized text at ({x}, {y}) with '{text}'\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"Error parsing 'solution' for id {item['id']}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing entry: {str(e)}\")\n",
    "\n",
    "    # Step 6: Save the modified PDF\n",
    "    try:\n",
    "        doc.save(output_pdf_path)\n",
    "        doc.close()\n",
    "        logging.info(f\"PDF modified and saved as {output_pdf_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving PDF: {str(e)}\")\n",
    "        doc.close()\n",
    "        shutil.rmtree(working_folder)\n",
    "        return\n",
    "\n",
    "    # Step 7: Clean up working folder\n",
    "    try:\n",
    "        shutil.rmtree(working_folder)\n",
    "        logging.info(f\"Deleted temporary folder: {working_folder}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting working folder: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d0d9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:10:43,755 [INFO] Applying OCR to ./Math_notation - Copy\\Math_notation - Copy.pdf...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">d:\\Users\\envs\\visualize\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "d:\\Users\\envs\\visualize\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:10:45,775 [INFO] skipping all processing on this page\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:10:45,798 [INFO] Postprocessing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">d:\\Users\\envs\\visualize\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "d:\\Users\\envs\\visualize\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 15:10:45,909 [INFO] Image optimization ratio: 1.00 savings: 0.0%\n",
      "2025-05-08 15:10:45,911 [INFO] Total file size ratio: 1.10 savings: 8.7%\n",
      "2025-05-08 15:10:45,953 [INFO] OCR completed: Math_notation - Copy\\Math_notation - Copy.ocr.pdf\n",
      "2025-05-08 15:10:45,953 [INFO] OCR PDF saved to Math_notation - Copy\\Math_notation - Copy.ocr.pdf\n",
      "2025-05-08 15:10:45,954 [INFO] Extracting cells from Math_notation - Copy\\Math_notation - Copy.ocr.pdf...\n",
      "2025-05-08 15:10:45,974 [INFO] Extracted 82 cells\n",
      "2025-05-08 15:10:45,979 [INFO] Saved 82 cells to ./Math_notation - Copy\\Math_notation - Copy.csv\n",
      "2025-05-08 15:10:45,983 [INFO] Opened ./Math_notation - Copy\\Math_notation - Copy.ocr.pdf, size: 612.0 x 792.0\n",
      "2025-05-08 15:10:46,022 [INFO] Read 1 rows from ./Math_notation - Copy\\Math_notation - Copy.csv\n",
      "2025-05-08 15:10:46,032 [INFO] Processing item with id: Math_notation - Copy, 82 entries\n",
      "2025-05-08 15:10:46,139 [INFO] Visualized text at (72.0, 72.99362182617188) with 'dominated by the unitary Hagedorn states is altered to be dominated by the orthogonal'\n",
      "2025-05-08 15:10:46,154 [INFO] Visualized text at (72.0, 93.87332153320312) with 'Hagedorn states when the dilute nuclear matter is heated up to higher temperatures. Since'\n",
      "2025-05-08 15:10:46,168 [INFO] Visualized text at (72.0, 114.87332153320312) with 'the mass spectral exponent for orthogonal Hagedorns (i.e. colorless orthogonal states) is'\n",
      "2025-05-08 15:10:46,176 [INFO] Visualized text at (72.0, 135.75344848632812) with 'found to be'\n",
      "2025-05-08 15:10:46,185 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:46,192 [INFO] Visualized text at (131.32058715820312, 135.75344848632812) with 'α'\n",
      "2025-05-08 15:10:46,200 [INFO] Visualized text at (142.80003356933594, 140.54225158691406) with '1'\n",
      "2025-05-08 15:10:46,217 [INFO] Visualized text at (147.03216552734375, 135.75344848632812) with '= 3, it is likely that the orthogonal Hagedorn matter undergoes third order'\n",
      "2025-05-08 15:10:46,232 [INFO] Visualized text at (72.00018310546875, 156.63357543945312) with 'phase transition to quark-gluon plasma. Furthermore, it is possible that the orthogonal'\n",
      "2025-05-08 15:10:46,243 [INFO] Visualized text at (72.00018310546875, 177.63357543945312) with 'Hagedorn states are altered to colorless'\n",
      "2025-05-08 15:10:46,250 [INFO] Visualized text at (270.53485107421875, 177.63357543945312) with 'U'\n",
      "2025-05-08 15:10:46,255 [INFO] Visualized text at (283.08013916015625, 177.63357543945312) with '(1)'\n",
      "2025-05-08 15:10:46,263 [INFO] Visualized text at (298.08013916015625, 176.3024444580078) with 'N'\n",
      "2025-05-08 15:10:46,269 [INFO] Visualized text at (304.800048828125, 178.6456298828125) with 'c'\n",
      "2025-05-08 15:10:46,279 [INFO] Visualized text at (312.4801330566406, 177.63357543945312) with 'states when the very dilute nuclear matter is'\n",
      "2025-05-08 15:10:46,297 [INFO] Visualized text at (72.00013732910156, 198.51370239257812) with 'further heated up to higher temperatures. The very dilute nuclear matter might be created'\n",
      "2025-05-08 15:10:46,303 [INFO] Visualized text at (72.00015258789062, 219.39340209960938) with 'in the'\n",
      "2025-05-08 15:10:46,310 [INFO] Visualized text at (102.64058685302734, 219.39340209960938) with 'pp'\n",
      "2025-05-08 15:10:46,323 [INFO] Visualized text at (119.03028869628906, 219.39340209960938) with 'collisions at LHC besides the heavy ion collisions. The Hagedorn matter which'\n",
      "2025-05-08 15:10:46,334 [INFO] Visualized text at (72.00006103515625, 240.39340209960938) with 'is dominated by the colorless'\n",
      "2025-05-08 15:10:46,343 [INFO] Visualized text at (226.61456298828125, 240.39340209960938) with 'U'\n",
      "2025-05-08 15:10:46,353 [INFO] Visualized text at (241.31997680664062, 240.39340209960938) with '(1)'\n",
      "2025-05-08 15:10:46,360 [INFO] Visualized text at (256.3199768066406, 239.06227111816406) with 'N'\n",
      "2025-05-08 15:10:46,369 [INFO] Visualized text at (263.0398864746094, 241.40582275390625) with 'c'\n",
      "2025-05-08 15:10:46,381 [INFO] Visualized text at (272.880126953125, 240.39334106445312) with 'has the mass spectral exponent'\n",
      "2025-05-08 15:10:46,381 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:46,390 [INFO] Visualized text at (439.1827697753906, 240.39334106445312) with 'α'\n",
      "2025-05-08 15:10:46,398 [INFO] Visualized text at (452.2752380371094, 240.39334106445312) with '= 3'\n",
      "2025-05-08 15:10:46,405 [INFO] Visualized text at (479.2789001464844, 240.39334106445312) with '/'\n",
      "2025-05-08 15:10:46,414 [INFO] Visualized text at (485.1589660644531, 240.39334106445312) with '2.'\n",
      "2025-05-08 15:10:46,420 [INFO] Visualized text at (504.11895751953125, 240.39334106445312) with 'Hence,'\n",
      "2025-05-08 15:10:46,436 [INFO] Visualized text at (71.99996948242188, 261.2734680175781) with 'the nuclear matter that is dominated by these states does not undergo direct abrupt phase'\n",
      "2025-05-08 15:10:46,451 [INFO] Visualized text at (71.99996948242188, 282.153564453125) with 'transition to quark-gluon plasma but rather smooth cross-over phase transition. When the'\n",
      "2025-05-08 15:10:46,464 [INFO] Visualized text at (71.99996948242188, 303.153564453125) with 'medium is further heated up to higher temperature these states (i.e. Hagedorn states with'\n",
      "2025-05-08 15:10:46,473 [INFO] Visualized text at (71.99996948242188, 324.0336608886719) with 'the mass spectral exponent'\n",
      "2025-05-08 15:10:46,474 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:46,482 [INFO] Visualized text at (213.46310424804688, 324.0336608886719) with 'α'\n",
      "2025-05-08 15:10:46,489 [INFO] Visualized text at (225.7161102294922, 324.0336608886719) with '= 3'\n",
      "2025-05-08 15:10:46,496 [INFO] Visualized text at (250.07997131347656, 324.0336608886719) with '/'\n",
      "2025-05-08 15:10:46,508 [INFO] Visualized text at (255.96005249023438, 324.0336608886719) with '2) may be mutated to metastable colored quark-gluon'\n",
      "2025-05-08 15:10:46,520 [INFO] Visualized text at (72.00004577636719, 345.0336608886719) with 'bags with the mass spectral exponent'\n",
      "2025-05-08 15:10:46,521 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:46,530 [INFO] Visualized text at (260.6229248046875, 345.0336608886719) with 'α'\n",
      "2025-05-08 15:10:46,538 [INFO] Visualized text at (271.4361877441406, 345.0336608886719) with '= 1'\n",
      "2025-05-08 15:10:46,546 [INFO] Visualized text at (293.1602478027344, 345.0336608886719) with '/'\n",
      "2025-05-08 15:10:46,560 [INFO] Visualized text at (299.0403137207031, 345.0336608886719) with '2. Since the states with mass spectral exponent'\n",
      "2025-05-08 15:10:46,562 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:46,570 [INFO] Visualized text at (72.00032043457031, 365.91375732421875) with 'α'\n",
      "2025-05-08 15:10:46,579 [INFO] Visualized text at (79.43645477294922, 365.91375732421875) with '= 1'\n",
      "2025-05-08 15:10:46,589 [INFO] Visualized text at (101.1605224609375, 365.91375732421875) with '/'\n",
      "2025-05-08 15:10:46,701 [INFO] Visualized text at (107.04060363769531, 365.91375732421875) with '2 do not pass direct explosive deconfinement phase transition to quark-gluon plasma,'\n",
      "2025-05-08 15:10:46,714 [INFO] Visualized text at (72.0006103515625, 386.7938537597656) with 'the colored quark-gluon bags expand smoothly and the system undergoes smooth phase'\n",
      "2025-05-08 15:10:46,724 [INFO] Visualized text at (72.0006103515625, 407.7938537597656) with 'transition to colored quark-gluon plasma.'\n",
      "2025-05-08 15:10:46,737 [INFO] Visualized text at (87.0006103515625, 428.6735534667969) with 'The orthogonal Hagedorn states are mutated to the colorless'\n",
      "2025-05-08 15:10:46,743 [INFO] Visualized text at (404.9359436035156, 428.6735534667969) with 'U'\n",
      "2025-05-08 15:10:46,752 [INFO] Visualized text at (418.800537109375, 428.6735534667969) with '(1)'\n",
      "2025-05-08 15:10:46,758 [INFO] Visualized text at (433.800537109375, 427.34246826171875) with 'N'\n",
      "2025-05-08 15:10:46,764 [INFO] Visualized text at (440.52044677734375, 429.6860046386719) with 'c'\n",
      "2025-05-08 15:10:46,772 [INFO] Visualized text at (449.640380859375, 428.6735534667969) with 'quark-gluon bags'\n",
      "2025-05-08 15:10:46,788 [INFO] Visualized text at (72.0003662109375, 449.55364990234375) with 'due to the high thermal excitations in the hot and very dilute nuclear matter (i.e.'\n",
      "2025-05-08 15:10:46,796 [INFO] Visualized text at (489.12017822265625, 449.55364990234375) with 'μ'\n",
      "2025-05-08 15:10:46,803 [INFO] Visualized text at (501.36102294921875, 454.34246826171875) with 'B'\n",
      "2025-05-08 15:10:46,812 [INFO] Visualized text at (507.7450866699219, 449.2547607421875) with '≈'\n",
      "2025-05-08 15:10:46,820 [INFO] Visualized text at (524.6407470703125, 449.55364990234375) with '0).'\n",
      "2025-05-08 15:10:46,835 [INFO] Visualized text at (72.000732421875, 470.55364990234375) with 'Since the new nuclear matter turns to be dominated by the colorless'\n",
      "2025-05-08 15:10:46,841 [INFO] Visualized text at (432.29534912109375, 470.55364990234375) with 'U'\n",
      "2025-05-08 15:10:46,850 [INFO] Visualized text at (446.2806701660156, 470.55364990234375) with '(1)'\n",
      "2025-05-08 15:10:46,857 [INFO] Visualized text at (461.2806701660156, 469.1026306152344) with 'N'\n",
      "2025-05-08 15:10:46,865 [INFO] Visualized text at (468.0005798339844, 471.44580078125) with 'c'\n",
      "2025-05-08 15:10:46,876 [INFO] Visualized text at (477.1205139160156, 470.55364990234375) with 'quark-gluon'\n",
      "2025-05-08 15:10:46,892 [INFO] Visualized text at (72.00051879882812, 491.4337463378906) with 'bags, it does not likely undergo direct phase transition to explosive quark-gluon plasma. But'\n",
      "2025-05-08 15:10:46,908 [INFO] Visualized text at (72.00051879882812, 512.3138427734375) with 'instead, the resultant Hagedorn states are gradually altered to metastable colored quark-'\n",
      "2025-05-08 15:10:46,921 [INFO] Visualized text at (72.00051879882812, 533.3138427734375) with 'gluon bubbles.'\n",
      "2025-05-08 15:10:46,938 [INFO] Visualized text at (157.9202423095703, 533.3138427734375) with 'The metastable colored quark-gluon bags expand gradually and overlap'\n",
      "2025-05-08 15:10:46,951 [INFO] Visualized text at (72.00051879882812, 554.1934814453125) with 'each other smoothly until the entire space is filled by giant colored (non-singlet) bags.'\n",
      "2025-05-08 15:10:46,965 [INFO] Visualized text at (72.00051879882812, 575.0736083984375) with 'The resultant matter have an initial neutral color charge aftermath the phase transition.'\n",
      "2025-05-08 15:10:46,981 [INFO] Visualized text at (72.00051879882812, 596.0736083984375) with 'Therefore, the constraints of the conserved color charges must be embedded in the system'\n",
      "2025-05-08 15:10:46,996 [INFO] Visualized text at (72.00051879882812, 616.9537353515625) with 'through the color chemical potentials. This kind of (color-non-singlet) matter with the mass'\n",
      "2025-05-08 15:10:47,005 [INFO] Visualized text at (72.00051879882812, 637.8338623046875) with 'spectral exponent'\n",
      "2025-05-08 15:10:47,007 [WARNING] Found unmapped special characters: {'α'}. Consider adding to unicode_mapping.\n",
      "2025-05-08 15:10:47,014 [INFO] Visualized text at (163.90338134765625, 637.8338623046875) with 'α'\n",
      "2025-05-08 15:10:47,021 [INFO] Visualized text at (177.12045288085938, 642.6226196289062) with 'non'\n",
      "2025-05-08 15:10:47,035 [INFO] Visualized text at (191.4932098388672, 637.8338623046875) with 'undergoes a smooth cross-over phase transition to non-explosive'\n",
      "2025-05-08 15:10:47,049 [INFO] Visualized text at (72.0003662109375, 658.8338623046875) with 'quark-gluon plasma. The multi-processes mechanism in the phase transition from the low-'\n",
      "2025-05-08 15:10:47,133 [INFO] Visualized text at (72.0003662109375, 679.7138671875) with 'lying hadronic phase to the quark-gluon plasma strongly indicates the fluid behaviour for the'\n",
      "2025-05-08 15:10:47,146 [INFO] Visualized text at (72.0003662109375, 700.5936279296875) with 'quark-gluon plasma. The color-singlet states for the quark-gluon bag with an orthogonal'\n",
      "2025-05-08 15:10:47,161 [INFO] Visualized text at (72.0003662109375, 721.5936279296875) with 'color representation rather than the unitary one can be interpreted as a gas of Coulomb'\n",
      "2025-05-08 15:10:47,169 [INFO] Visualized text at (293.400390625, 755.9139404296875) with '52'\n",
      "2025-05-08 15:10:47,188 [INFO] PDF modified and saved as ./translation_Math_notation - Copy.pdf\n",
      "2025-05-08 15:10:47,190 [INFO] Deleted temporary folder: ./Math_notation - Copy\n"
     ]
    }
   ],
   "source": [
    "document_translation('Math_notation - Copy.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62c65e",
   "metadata": {},
   "source": [
    "# Math Equation Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c342056",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b5bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, gc, shutil, yaml\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from pdf2image import convert_from_path\n",
    "import fitz  # PyMuPDF\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df91e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (2048, 1447)\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.75\n",
    "FONT_THICKNESS = 2\n",
    "BORDER_THICKNESS = 2\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "INPUT_SIZE = 1024\n",
    "N_EPOCHS = 15\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 4\n",
    "CACHE_DATA = True\n",
    "DEVICES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b55d90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = \"best.pt\"\n",
    "best_model = YOLO(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "064bde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_jpg_with_sizes(pdf_path, output_folder, dpi=300):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Convert PDF to images\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "    # Get PDF page sizes using PyMuPDF\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Use first page for size.txt (assuming all pages same size)\n",
    "    first_image = images[0]\n",
    "    first_page = pdf_doc[0]\n",
    "\n",
    "    # Get sizes\n",
    "    jpg_size = first_image.size  # (width, height) in pixels\n",
    "    pdf_size = (first_page.rect.width, first_page.rect.height)  # (width, height) in points\n",
    "\n",
    "    # Write to size.txt\n",
    "    size_txt_path = os.path.join(output_folder, 'size.txt')\n",
    "    with open(size_txt_path, 'w') as f:\n",
    "        f.write(f\"{jpg_size}\\n\")\n",
    "        f.write(f\"{pdf_size}\\n\")\n",
    "    print(f\"Saved size.txt at {size_txt_path}\")\n",
    "\n",
    "    # Save JPGs and print sizes\n",
    "    for i, (image, page) in enumerate(zip(images, pdf_doc)):\n",
    "        jpg_path = os.path.join(output_folder, f'{output_folder}.jpg')\n",
    "        image.save(jpg_path, 'JPEG')\n",
    "\n",
    "        print(f'Page {i+1}: PDF size = {pdf_size[0]} x {pdf_size[1]} pt, JPG size = {jpg_size[0]} x {jpg_size[1]} px')\n",
    "        print(f'Saved: {jpg_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "887e9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_box_to_pdf(jpg_box, jpg_size, pdf_size):\n",
    "    x1, y1, x2, y2 = jpg_box\n",
    "    jpg_width, jpg_height = jpg_size\n",
    "    pdf_width, pdf_height = pdf_size\n",
    "\n",
    "    scale_x = pdf_width / jpg_width\n",
    "    scale_y = pdf_height / jpg_height\n",
    "\n",
    "    scaled_x1 = x1 * scale_x\n",
    "    scaled_y1 = y1 * scale_y\n",
    "    scaled_x2 = x2 * scale_x\n",
    "    scaled_y2 = y2 * scale_y\n",
    "\n",
    "    return [scaled_x1, scaled_y1, scaled_x2, scaled_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3443205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pdf_coordinates(image_folder):\n",
    "    \"\"\"\n",
    "    Generate pdf_coor.txt with PDF-scaled coordinates from index.txt and size.txt.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Folder containing index.txt, size.txt, and images.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "\n",
    "    # Load size.txt\n",
    "    size_path = os.path.join(image_folder, 'size.txt')\n",
    "    with open(size_path, 'r') as f:\n",
    "        jpg_size = ast.literal_eval(f.readline().strip())\n",
    "        pdf_size = ast.literal_eval(f.readline().strip())\n",
    "\n",
    "    # Load index.txt\n",
    "    index_path = os.path.join(image_folder, 'index.txt')\n",
    "    with open(index_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    pdf_coordinates = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue  # skip bad lines\n",
    "\n",
    "        box_id, x1, y1, x2, y2 = parts\n",
    "        x1, y1, x2, y2 = map(float, [x1, y1, x2, y2])\n",
    "\n",
    "        # Convert coordinates to PDF space\n",
    "        scaled_box = scale_box_to_pdf([x1, y1, x2, y2], jpg_size, pdf_size)\n",
    "\n",
    "        # Format line for output\n",
    "        pdf_line = f\"{box_id} {scaled_box[0]:.4f} {scaled_box[1]:.4f} {scaled_box[2]:.4f} {scaled_box[3]:.4f}\"\n",
    "        pdf_coordinates.append(pdf_line)\n",
    "\n",
    "    # Save to pdf_coor.txt\n",
    "    pdf_coor_path = os.path.join(image_folder, 'pdf_coor.txt')\n",
    "    with open(pdf_coor_path, 'w') as f:\n",
    "        for line in pdf_coordinates:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    print(f\"PDF coordinates saved at: {pdf_coor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47b58ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_normalize_all(root_folder):\n",
    "    \"\"\"\n",
    "    Process all .jpg and .txt pairs in root folder, crop boxes, rename txt files,\n",
    "    and save normalized data with float coordinates.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): Root folder containing .jpg and .txt files.\n",
    "    \"\"\"\n",
    "    # Create images folder\n",
    "    images_folder = os.path.join(root_folder, 'images')\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "    # Find all jpg files\n",
    "    jpg_files = glob.glob(os.path.join(root_folder, '*.jpg'))\n",
    "\n",
    "    for jpg_path in jpg_files:\n",
    "        base_name = os.path.splitext(os.path.basename(jpg_path))[0]\n",
    "        txt_path = os.path.join(root_folder, f'{base_name}.txt')\n",
    "\n",
    "        if not os.path.exists(txt_path):\n",
    "            print(f'Skipping {base_name}: no matching txt file.')\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(jpg_path)\n",
    "        h_img, w_img = image.shape[:2]\n",
    "\n",
    "        # Read txt file\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        normalized_lines = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue  # skip bad lines\n",
    "\n",
    "            box_id = i + 1\n",
    "            x1, y1, x2, y2 = map(float, parts[1:])\n",
    "\n",
    "            # Apply ±5 adjustment\n",
    "            y1_adj = y1 - 5\n",
    "            y2_adj = y2 + 5\n",
    "\n",
    "            # Clamp coordinates within image boundaries\n",
    "            x1_clamped = max(0.0, x1)\n",
    "            y1_clamped = max(0.0, y1_adj)\n",
    "            x2_clamped = min(float(w_img), x2)\n",
    "            y2_clamped = min(float(h_img), y2_adj)\n",
    "\n",
    "            # Crop image using int for pixel slicing\n",
    "            crop = image[int(y1_clamped):int(y2_clamped), int(x1_clamped):int(x2_clamped)]\n",
    "\n",
    "            # Save as images/id.jpg (only id, no prefix)\n",
    "            crop_filename = f'{box_id}.jpg'\n",
    "            crop_path = os.path.join(images_folder, crop_filename)\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            print(f'Saved: {crop_path}')\n",
    "\n",
    "            # Save normalized line with float precision (4 decimal places)\n",
    "            normalized_line = f\"{box_id} {x1_clamped:.4f} {y1_clamped:.4f} {x2_clamped:.4f} {y2_clamped:.4f}\"\n",
    "            normalized_lines.append(normalized_line)\n",
    "\n",
    "        # # Rename original txt → conf.txt (no prefix)\n",
    "        # conf_txt_path = os.path.join(root_folder, 'conf.txt')\n",
    "        # os.rename(txt_path, conf_txt_path)\n",
    "        # print(f'Renamed {txt_path} → {conf_txt_path}')\n",
    "\n",
    "        # Save normalized txt as index.txt (no prefix)\n",
    "        index_txt_path = os.path.join(root_folder, 'index.txt')\n",
    "        with open(index_txt_path, 'w') as f:\n",
    "            for line in normalized_lines:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        generate_pdf_coordinates(root_folder)\n",
    "\n",
    "        print(f'Index txt saved at: {index_txt_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ee9a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_root = 'Math_notation - Copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c7f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved size.txt at Math_notation - Copy\\size.txt\n",
      "Page 1: PDF size = 612.0 x 792.0 pt, JPG size = 2550 x 3300 px\n",
      "Saved: Math_notation - Copy\\Math_notation - Copy.jpg\n"
     ]
    }
   ],
   "source": [
    "pdf_to_jpg_with_sizes(name_root + '.pdf', name_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b777af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_ROOT = './predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92ebccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = best_model.predict(\n",
    "        source= './' + name_root,\n",
    "        conf=0.65,\n",
    "        iou=0.75,\n",
    "        stream=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adddd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "\n",
    "for prediction in predictions:\n",
    "    if len(prediction.boxes.xyxy):\n",
    "        name = prediction.path.split(\"/\")[-1].split(\".\")[0]\n",
    "        boxes = prediction.boxes.xyxy.cpu().numpy()\n",
    "        scores = prediction.boxes.conf.cpu().numpy()\n",
    "        \n",
    "        test_images += [name]\n",
    "        label_path = os.path.join(PREDICTIONS_ROOT, name + \".txt\")\n",
    "        \n",
    "        with open(label_path, \"w+\") as f:\n",
    "            for score, box in zip(scores, boxes):\n",
    "                text = f\"{score:0.4f} {' '.join(box.astype(str))}\"\n",
    "                f.write(text)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ede8da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Math_notation - Copy\\images\\1.jpg\n",
      "Saved: Math_notation - Copy\\images\\2.jpg\n",
      "Saved: Math_notation - Copy\\images\\3.jpg\n",
      "Saved: Math_notation - Copy\\images\\4.jpg\n",
      "Saved: Math_notation - Copy\\images\\5.jpg\n",
      "Saved: Math_notation - Copy\\images\\6.jpg\n",
      "Saved: Math_notation - Copy\\images\\7.jpg\n",
      "Saved: Math_notation - Copy\\images\\8.jpg\n",
      "Saved: Math_notation - Copy\\images\\9.jpg\n",
      "Saved: Math_notation - Copy\\images\\10.jpg\n",
      "Saved: Math_notation - Copy\\images\\11.jpg\n",
      "PDF coordinates saved at: Math_notation - Copy\\pdf_coor.txt\n",
      "Index txt saved at: Math_notation - Copy\\index.txt\n"
     ]
    }
   ],
   "source": [
    "crop_and_normalize_all(name_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a26c5",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17fdc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import os\n",
    "import ast  # To safely parse the tuple from size.txt\n",
    "from PyPDF2 import PdfReader, PdfWriter, Transformation\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89c8a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_box_to_pdf(jpg_box, jpg_size, pdf_size):\n",
    "    x1, y1, x2, y2 = jpg_box\n",
    "    jpg_width, jpg_height = jpg_size\n",
    "    pdf_width, pdf_height = pdf_size\n",
    "\n",
    "    scale_x = pdf_width / jpg_width\n",
    "    scale_y = pdf_height / jpg_height\n",
    "\n",
    "    scaled_x1 = x1 * scale_x\n",
    "    scaled_y1 = y1 * scale_y\n",
    "    scaled_x2 = x2 * scale_x\n",
    "    scaled_y2 = y2 * scale_y\n",
    "\n",
    "    return [scaled_x1, scaled_y1, scaled_x2, scaled_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "078594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_images(pdf_path, image_folder):\n",
    "    \"\"\"\n",
    "    Insert cropped images back into the original PDF at scaled coordinates.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF.\n",
    "        image_folder (str): Folder containing index.txt, size.txt, and cropped images folder ('images').\n",
    "    \"\"\"\n",
    "    # Load size.txt\n",
    "    size_path = os.path.join(image_folder, 'size.txt')\n",
    "    with open(size_path, 'r') as f:\n",
    "        jpg_size = ast.literal_eval(f.readline().strip())\n",
    "        pdf_size = ast.literal_eval(f.readline().strip())\n",
    "\n",
    "    # Load index.txt\n",
    "    index_path = os.path.join(image_folder, 'index.txt')\n",
    "    with open(index_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Open PDF\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_pdf = f\"{pdf_name}_insert_images.pdf\"\n",
    "\n",
    "    # Use first page (or extend later if needed)\n",
    "    page = pdf_doc[0]\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue\n",
    "\n",
    "        img_id, x1, y1, x2, y2 = parts\n",
    "        img_id = int(img_id)\n",
    "        x1, y1, x2, y2 = map(float, [x1, y1, x2, y2])\n",
    "\n",
    "        img_path = os.path.join(image_folder, f\"images/{img_id}.jpg\")\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Scale coordinates\n",
    "        scaled_box = scale_box_to_pdf([x1, y1, x2, y2], jpg_size, pdf_size)\n",
    "        rect = fitz.Rect(scaled_box)\n",
    "\n",
    "        # Insert image\n",
    "        page.insert_image(rect, filename=img_path)\n",
    "\n",
    "    # Save PDF\n",
    "    pdf_doc.save(output_pdf)\n",
    "    pdf_doc.close()\n",
    "    print(f\"Saved output PDF as {output_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006ebe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pdf(input_path, output_path, target_width=1025, target_height=1025):\n",
    "    \"\"\"\n",
    "    Scale a PDF to the target dimensions ensuring both page size and content are scaled.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input PDF file\n",
    "        output_path (str): Path where the scaled PDF will be saved\n",
    "        target_width (int): Target width in pixels (default: 1025)\n",
    "        target_height (int): Target height in pixels (default: 1025)\n",
    "    \"\"\"\n",
    "    # Read the original PDF\n",
    "    reader = PdfReader(input_path)\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # Convert target dimensions from pixels to points (72 points = 1 inch)\n",
    "    # Assuming 72 DPI resolution\n",
    "    target_width_pts = target_width\n",
    "    target_height_pts = target_height\n",
    "\n",
    "    # Process each page\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        # Get the original page\n",
    "        original_page = reader.pages[page_num]\n",
    "\n",
    "        # Get original page dimensions\n",
    "        mediabox = original_page.mediabox\n",
    "        orig_width = float(mediabox.width)\n",
    "        orig_height = float(mediabox.height)\n",
    "\n",
    "        # Calculate scaling factors\n",
    "        width_scale = target_width_pts / orig_width\n",
    "        height_scale = target_height_pts / orig_height\n",
    "\n",
    "        # Create a copy of the page to work with\n",
    "        page = copy.deepcopy(original_page)\n",
    "\n",
    "        # Apply scaling transformation to the content\n",
    "        transform = Transformation().scale(width_scale, height_scale)\n",
    "        page.add_transformation(transform)\n",
    "\n",
    "        # Update the mediabox to the new dimensions\n",
    "        # PyPDF2 uses a coordinate system with (0,0) at the bottom left\n",
    "        page.mediabox.lower_left = (0, 0)\n",
    "        page.mediabox.upper_right = (target_width_pts, target_height_pts)\n",
    "\n",
    "        # Also update cropbox and trimbox if they exist\n",
    "        if \"/CropBox\" in page:\n",
    "            page.cropbox.lower_left = (0, 0)\n",
    "            page.cropbox.upper_right = (target_width_pts, target_height_pts)\n",
    "\n",
    "        if \"/TrimBox\" in page:\n",
    "            page.trimbox.lower_left = (0, 0)\n",
    "            page.trimbox.upper_right = (target_width_pts, target_height_pts)\n",
    "\n",
    "        if \"/ArtBox\" in page:\n",
    "            page.artbox.lower_left = (0, 0)\n",
    "            page.artbox.upper_right = (target_width_pts, target_height_pts)\n",
    "\n",
    "        if \"/BleedBox\" in page:\n",
    "            page.bleedbox.lower_left = (0, 0)\n",
    "            page.bleedbox.upper_right = (target_width_pts, target_height_pts)\n",
    "\n",
    "        # Add the scaled page to the output PDF\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Write the result to the output file\n",
    "    with open(output_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "    print(f\"PDF scaled successfully to {target_width}x{target_height}.\")\n",
    "    print(f\"Both page dimensions and content have been scaled. Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2277c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pdf_from_folder(pdf_name, folder_name):\n",
    "    \"\"\"\n",
    "    Read target dimensions from size.txt, check current PDF size, and rescale if needed.\n",
    "    Save scaled PDF in current directory.\n",
    "    \"\"\"\n",
    "    size_file_path = os.path.join(folder_name, 'size.txt')\n",
    "    pdf_input_path = pdf_name\n",
    "    pdf_output_path = f\"{os.path.splitext(pdf_name)[0]}_scale.pdf\"\n",
    "\n",
    "    # Read target dimensions\n",
    "    with open(size_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) < 2:\n",
    "            raise ValueError(\"size.txt does not contain at least two lines.\")\n",
    "        size_line = lines[1].strip()\n",
    "        try:\n",
    "            target_width, target_height = eval(size_line)\n",
    "            if not (isinstance(target_width, (int, float)) and isinstance(target_height, (int, float))):\n",
    "                raise ValueError(\"Size values must be numbers.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid size tuple in size.txt: {size_line}\") from e\n",
    "\n",
    "    # Check current PDF size (first page)\n",
    "    reader = PdfReader(pdf_input_path)\n",
    "    first_page = reader.pages[0]\n",
    "    orig_width = float(first_page.mediabox.width)\n",
    "    orig_height = float(first_page.mediabox.height)\n",
    "\n",
    "    # Compare with target size (allow tiny tolerance)\n",
    "    tolerance = 0.01\n",
    "    if abs(orig_width - target_width) < tolerance and abs(orig_height - target_height) < tolerance:\n",
    "        print(f\"No scaling needed. PDF already matches target size {target_width}x{target_height}.\")\n",
    "    else:\n",
    "        scale_pdf(pdf_input_path, pdf_output_path, target_width, target_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "048f07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_images(pdf_path, image_folder):\n",
    "    \"\"\"\n",
    "    Insert cropped images back into the PDF, rescaling the PDF first if needed.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF.\n",
    "        image_folder (str): Folder containing pdf_coor.txt, size.txt, and cropped images folder ('images').\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    import os\n",
    "    from PyPDF2 import PdfReader\n",
    "    import fitz\n",
    "\n",
    "    # Load size.txt\n",
    "    size_path = os.path.join(image_folder, 'size.txt')\n",
    "    with open(size_path, 'r') as f:\n",
    "        jpg_size = ast.literal_eval(f.readline().strip())\n",
    "        pdf_size = ast.literal_eval(f.readline().strip())\n",
    "    target_width, target_height = pdf_size\n",
    "\n",
    "    # Check current PDF size\n",
    "    reader = PdfReader(pdf_path)\n",
    "    first_page = reader.pages[0]\n",
    "    orig_width = float(first_page.mediabox.width)\n",
    "    orig_height = float(first_page.mediabox.height)\n",
    "\n",
    "    tolerance = 0.01\n",
    "    if abs(orig_width - target_width) < tolerance and abs(orig_height - target_height) < tolerance:\n",
    "        scaled_pdf_path = pdf_path\n",
    "        print(\"PDF size matches target. Proceeding to insert images.\")\n",
    "    else:\n",
    "        print(\"PDF size does not match target. Rescaling PDF first...\")\n",
    "        folder_name = image_folder\n",
    "        pdf_name = os.path.basename(pdf_path)\n",
    "        scale_pdf_from_folder(pdf_name, folder_name)\n",
    "        scaled_pdf_path = f\"{os.path.splitext(pdf_name)[0]}_scale.pdf\"\n",
    "\n",
    "    # Load pdf_coor.txt\n",
    "    pdf_coor_path = os.path.join(image_folder, 'pdf_coor.txt')\n",
    "    if not os.path.exists(pdf_coor_path):\n",
    "        raise FileNotFoundError(f\"pdf_coor.txt not found in folder: {image_folder}\")\n",
    "\n",
    "    with open(pdf_coor_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Open scaled PDF\n",
    "    pdf_doc = fitz.open(scaled_pdf_path)\n",
    "    pdf_name = os.path.splitext(os.path.basename(scaled_pdf_path))[0]\n",
    "    output_pdf = f\"{pdf_name}_insert_images.pdf\"\n",
    "\n",
    "    # Use first page (or extend later if needed)\n",
    "    page = pdf_doc[0]\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue\n",
    "\n",
    "        img_id, x1, y1, x2, y2 = parts\n",
    "        img_id = int(img_id)\n",
    "        x1, y1, x2, y2 = map(float, [x1, y1, x2, y2])\n",
    "\n",
    "        img_path = os.path.join(image_folder, f\"images/{img_id}.jpg\")\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        rect = fitz.Rect(x1 , y1 , x2 , y2 )\n",
    "\n",
    "        print(rect)\n",
    "\n",
    "        # Insert image\n",
    "        page.insert_image(rect, filename=img_path)\n",
    "        #page.draw_rect(rect, color=(1, 0, 0), fill=None, width=1)\n",
    "\n",
    "    # Save PDF\n",
    "    pdf_doc.save(output_pdf)\n",
    "    pdf_doc.close()\n",
    "    print(f\"Saved output PDF as {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26f0f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF size matches target. Proceeding to insert images.\n",
      "Rect(408.9157, 426.7901, 444.9264, 442.3542)\n",
      "Rect(436.7488, 468.8775, 472.039, 484.2936)\n",
      "Rect(231.5094, 238.413, 267.4838, 254.2285)\n",
      "Rect(273.5326, 175.9192, 308.594, 191.234)\n",
      "Rect(217.7661, 323.063, 261.6181, 337.4784)\n",
      "Rect(443.7224, 239.3031, 490.2527, 253.9552)\n",
      "Rect(263.7803, 344.3717, 304.2116, 358.4625)\n",
      "Rect(72.212, 365.2914, 112.7351, 379.6377)\n",
      "Rect(134.7101, 136.0891, 168.9302, 148.2758)\n",
      "Rect(169.0868, 641.018, 191.3845, 650.917)\n",
      "Rect(492.7863, 449.6114, 530.1297, 462.7044)\n",
      "Saved output PDF as translation_Math_notation - Copy_insert_images.pdf\n"
     ]
    }
   ],
   "source": [
    "insert_images('translation_Math_notation - Copy.pdf', 'Math_notation - Copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366fafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
