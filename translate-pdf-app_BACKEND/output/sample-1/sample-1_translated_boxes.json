[
    {
        "id": 4,
        "label": 4,
        "coords": [
            1326.685546875,
            1405.5018310546875,
            2330.6953125,
            1678.2056884765625
        ],
        "content": "Figure 1: The phenomenon of \\textit{good teachers} cannot always\\textit{teach good students} in KGEs based on KD: with the in-crease of dimension (from 256 to 1024) of teacher T, theperformances of students (S-64 denotes 64 dimensions andS-32 denotes 32 dimensions) drop. Results are obtained onWN18RR dataset with TransE.",
        "translation": "Hình 1: Hiện tượng \\textit{giáo viên giỏi} không phải lúc nào cũng có thể \\textit{dạy học sinh giỏi} trong KGE dựa trên KD: với sự gia tăng kích thước (từ 256 lên 1024) của giáo viên T, hiệu suất của học sinh (S-64 biểu thị 64 chiều và S-32 biểu thị 32 chiều) giảm. Kết quả thu được trên tập dữ liệu WN18RR với TransE.",
        "page_num": 0
    },
    {
        "id": 2,
        "label": 1,
        "coords": [
            222.92788696289062,
            2366.869140625,
            1222.3482666015625,
            2778.2490234375
        ],
        "content": "Knowledge graphs (KGs) describe concepts and facts ingraph models (Dong et al. 2014), where knowledge isstored as triples. With the increasing sizes of KGs such asWikipedia (Bizer et al. 2009) and Yago (Suchanek, Kas-neci, and Weikum 2007), efficient knowledge graph embed-ding (KGE), which embeds triples into a continuous vectorspace, plays a pivotal role in downstream applications suchas question answering (Bordes, Weston, and Usunier 2014),recommendation system (Zhang et al. 2016) and knowledge",
        "translation": "Đồ thị tri thức (Knowledge Graphs - KGs) mô tả các khái niệm và sự kiện trong các mô hình đồ thị (Dong et al. 2014), trong đó tri thức được lưu trữ dưới dạng các bộ ba. Với kích thước ngày càng tăng của KGs như Wikipedia (Bizer et al. 2009) và Yago (Suchanek, Kas-neci, and Weikum 2007), việc nhúng đồ thị tri thức (knowledge graph embedding - KGE) hiệu quả, giúp nhúng các bộ ba vào một không gian vectơ liên tục, đóng một vai trò then chốt trong các ứng dụng hạ nguồn như trả lời câu hỏi (Bordes, Weston, and Usunier 2014), hệ thống đề xuất (Zhang et al. 2016) và tri thức",
        "page_num": 0
    },
    {
        "id": 8,
        "label": 0,
        "coords": [
            584.9790649414062,
            2299.2138671875,
            858.82275390625,
            2352.2666015625
        ],
        "content": "Introduction",
        "translation": "Giới thiệu",
        "page_num": 0
    },
    {
        "id": 7,
        "label": 0,
        "coords": [
            266.5947570800781,
            397.30657958984375,
            2282.413818359375,
            535.0564575195312
        ],
        "content": "\\textbf{IterDE: An Iterative Knowledge Distillation Framework for Knowledge Graph}\\vspace{0.2cm}\\begin{center}\\textbf{Embeddings}\\end{center}",
        "translation": "\\textbf{IterDE: Một Khung Chắt Lọc Kiến Thức Lặp Đi Lặp Lại cho Đồ Thị Tri Thức}\\vspace{0.2cm}\\begin{center}\\textbf{Nhúng}\\end{center}",
        "page_num": 0
    },
    {
        "id": 3,
        "label": 1,
        "coords": [
            1329.55419921875,
            2706.991943359375,
            2328.56689453125,
            2936.821044921875
        ],
        "content": "Knowledge distillation (KD) is a popular technique formodel compression, where a larger model serves as a teachermodel, and a smaller model as a student model tries tomimic the output of the teacher model (Hinton et al. 2014).Recently, although several compression methods for KGES",
        "translation": "Chưng cất tri thức (KD) là một kỹ thuật phổ biến để nén mô hình, trong đó một mô hình lớn hơn đóng vai trò là mô hình giáo viên, và một mô hình nhỏ hơn đóng vai trò là mô hình học sinh cố gắng bắt chước đầu ra của mô hình giáo viên (Hinton et al. 2014). Gần đây, mặc dù có một số phương pháp nén cho KGES",
        "page_num": 0
    },
    {
        "id": 9,
        "label": 0,
        "coords": [
            644.40966796875,
            906.283447265625,
            803.3139038085938,
            951.40380859375
        ],
        "content": "Abstract",
        "translation": "Tóm tắt",
        "page_num": 0
    },
    {
        "id": 0,
        "label": 1,
        "coords": [
            1328.198974609375,
            1786.47900390625,
            2328.41943359375,
            2700.03125
        ],
        "content": "graph completion (Lin et al. 2015). Most KGE models such as TransE (Bordes et al. 2013), ComplEx (Trouillon et al. 2016), SimplE (Kazemi and Poole 2018), RotatE (Sun et al. 2019) have shown better performances with higher embedding dimensions and larger model sizes, however, that also leads to slower inference efficiency for practical applications. Specifically, the 512-dimensional KGE models have 7-15 times more embedding layer parameters and 2-6 times more inference time than the 32-dimensional KGE models (Zhu et al. 2022). Therefore, it is a nontrivial problem to compress KGEs from high-dimensional teacher models to low-dimensional student models while maintaining excellent performance. In realistic applications, KGE models are often required to simultaneously keep high performance and fast inference speed. For example, financial investors need to get accurate and fast market decision aids from financial KGs via edge devices. In this scenario, KGE models can be compressed by knowledge distillation and then deployed to edge devices to help financial investors make faster and more accurate decisions.",
        "translation": "hoàn thiện đồ thị (Lin và cộng sự 2015). Hầu hết các mô hình KGE như TransE (Bordes và cộng sự 2013), ComplEx (Trouillon và cộng sự 2016), SimplE (Kazemi và Poole 2018), RotatE (Sun và cộng sự 2019) đã cho thấy hiệu suất tốt hơn với số chiều nhúng cao hơn và kích thước mô hình lớn hơn, tuy nhiên, điều đó cũng dẫn đến hiệu quả suy luận chậm hơn cho các ứng dụng thực tế. Cụ thể, các mô hình KGE 512 chiều có số lượng tham số lớp nhúng nhiều hơn 7-15 lần và thời gian suy luận nhiều hơn 2-6 lần so với các mô hình KGE 32 chiều (Zhu và cộng sự 2022). Do đó, việc nén KGE từ các mô hình giáo viên chiều cao xuống các mô hình học sinh chiều thấp trong khi vẫn duy trì hiệu suất tuyệt vời là một vấn đề không hề nhỏ. Trong các ứng dụng thực tế, các mô hình KGE thường được yêu cầu đồng thời duy trì hiệu suất cao và tốc độ suy luận nhanh. Ví dụ, các nhà đầu tư tài chính cần nhận được các hỗ trợ quyết định thị trường nhanh chóng và chính xác từ KG tài chính thông qua các thiết bị biên. Trong kịch bản này, các mô hình KGE có thể được nén bằng cách chưng cất kiến thức và sau đó triển khai đến các thiết bị biên để giúp các nhà đầu tư tài chính đưa ra quyết định nhanh hơn và chính xác hơn.",
        "page_num": 0
    },
    {
        "id": 14,
        "label": 1,
        "coords": [
            706.9918823242188,
            589.1415405273438,
            1838.4156494140625,
            765.4775390625
        ],
        "content": "\\noindent Jiajun Liu, Peng Wang*, Ziyu Shang, Chenxiao Wu\\noindent School of Computer Science and Engineering, Southeast University\\noindent \\{jiajliu, pwang, ziyus1999, chenxiaowu\\}@seu.edu.cn",
        "translation": "\\noindent Jiajun Liu, Peng Wang*, Ziyu Shang, Chenxiao Wu\\noindent Trường Khoa học và Kỹ thuật Máy tính, Đại học Đông Nam\\noindent \\{jiajliu, pwang, ziyus1999, chenxiaowu\\}@seu.edu.cn",
        "page_num": 0
    },
    {
        "id": 1,
        "label": 1,
        "coords": [
            262.79046630859375,
            979.4585571289062,
            1182.4075927734375,
            2232.212890625
        ],
        "content": "Knowledge distillation for knowledge graph embedding(KGE) aims to reduce the KGE model size to address thechallenges of storage limitations and knowledge reasoningefficiency. However, current work still suffers from perfor-mance drops when compressing a high-dimensional originalKGE model to a low-dimensional distillation KGE model.Moreover, most work focuses on the reduction of inferencetime but ignores the time-consuming training process of dis-tilling KGE models. In this paper, we propose IterDE, a novelknowledge distillation framework for KGEs. First, IterDEintroduces an iterative distillation way and enables a KGEmodel to alternately be a student model and a teacher modelduring the iterative distillation process. Consequently, knowl-edge can be transferred in a smooth manner between high-dimensional teacher models and low-dimensional studentmodels, while preserving good KGE performances. Further-more, in order to optimize the training process, we considerthat different optimization objects between hard label lossand soft label loss can affect the efficiency of training, andthen we propose a soft-label weighting dynamic adjustmentmechanism that can balance the inconsistency of optimiza-tion direction between hard and soft label loss by graduallyincreasing the weighting of soft label loss. Our experimen-tal results demonstrate that IterDE achieves a new state-of-the-art distillation performance for KGEs compared to strongbaselines on the link prediction task. Significantly, IterDE canreduce the training time by 50\\% on average. Finally, moreexploratory experiments show that the soft-label weightingdynamic adjustment mechanism and more fine-grained itera-tions can improve distillation performance.",
        "translation": "Chưng cất tri thức cho việc nhúng đồ thị tri thức (KGE) nhằm mục đích giảm kích thước mô hình KGE để giải quyết các thách thức về giới hạn lưu trữ và hiệu quả suy luận tri thức. Tuy nhiên, các công trình hiện tại vẫn gặp phải tình trạng giảm hiệu suất khi nén một mô hình KGE gốc có chiều cao xuống một mô hình KGE chưng cất có chiều thấp. Hơn nữa, hầu hết các công trình tập trung vào việc giảm thời gian suy luận nhưng bỏ qua quá trình huấn luyện tốn thời gian của các mô hình KGE chưng cất. Trong bài báo này, chúng tôi đề xuất IterDE, một khung chưng cất tri thức mới cho KGE. Đầu tiên, IterDE giới thiệu một phương pháp chưng cất lặp đi lặp lại và cho phép một mô hình KGE luân phiên trở thành một mô hình học sinh và một mô hình giáo viên trong quá trình chưng cất lặp đi lặp lại. Do đó, tri thức có thể được chuyển giao một cách trơn tru giữa các mô hình giáo viên có chiều cao và các mô hình học sinh có chiều thấp, đồng thời vẫn duy trì hiệu suất KGE tốt. Hơn nữa, để tối ưu hóa quá trình huấn luyện, chúng tôi xem xét rằng các đối tượng tối ưu hóa khác nhau giữa mất mát nhãn cứng và mất mát nhãn mềm có thể ảnh hưởng đến hiệu quả huấn luyện, và sau đó chúng tôi đề xuất một cơ chế điều chỉnh động trọng số nhãn mềm có thể cân bằng sự không nhất quán của hướng tối ưu hóa giữa mất mát nhãn cứng và nhãn mềm bằng cách tăng dần trọng số của mất mát nhãn mềm. Kết quả thử nghiệm của chúng tôi chứng minh rằng IterDE đạt được hiệu suất chưng cất hiện đại mới cho KGE so với các đường cơ sở mạnh mẽ trên tác vụ dự đoán liên kết. Đáng chú ý, IterDE có thể giảm thời gian huấn luyện trung bình 50\\%. Cuối cùng, nhiều thử nghiệm thăm dò hơn cho thấy rằng cơ chế điều chỉnh động trọng số nhãn mềm và các lần lặp chi tiết hơn có thể cải thiện hiệu suất chưng cất.",
        "page_num": 0
    }
]